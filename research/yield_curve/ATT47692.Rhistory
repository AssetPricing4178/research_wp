q()
setwd("C:\Users\Jonat\Desktop\Lab 4")
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(stats)
library(ggplot2)
library(dplyr)
smhi <- read.csv("C:/Users/Jonat/Desktop/Lab 4/smhi_daily.csv", sep = ";")
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
smhi <- read.csv("C:/Users/Jonat/Desktop/Lab 4/smhi_daily.csv", sep = ";")
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
library(readr)
smhi_daily <- read_csv("C:/Users/Jonat/Desktop/Lab 4/smhi_daily.csv")
View(smhi_daily)
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
pacf(smhi_daily, main = "PACF för smhi")
# avtrendifiera tidserien
detrended_smhi <- diff(smhi_daily$DailyAverage)
plot(detrended_smhi, type = 'l')
# Convert the detrended data to a time series object
# Adjust the frequency parameter based on your data's seasonality
ts_data <- ts(detrended_smhi, frequency = 365)
# Split the data into training and testing sets
# Assuming you want to leave out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Uncomment these lines once you are ready to model
# fit <- auto.arima(train_data)
# checkresiduals(fit)
# Convert the detrended data to a time series object
# Adjust the frequency parameter based on your data's seasonality
ts_data <- ts(detrended_smhi, frequency = 365)
# Split the data into training and testing sets
# Assuming you want to leave out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Uncomment these lines once you are ready to model
fit <- auto.arima(train_data)
checkresiduals(fit)
checkresiduals(fit)
sarima_fit <- auto.arima(train_data, seasonal = TRUE)
checkresiduals(sarima_fit)
checkresiduals(fit)
# Utskrift av den valda modellen
summary(sarima_fit)
# Verifiera den valda modellen med diagnostiska plotter
checkresiduals(sarima_fit)
# Förutsäg framåt med den valda modellen
forecast_result <- forecast(sarima_fit, h = 14)
# Plotta prognosen med konfidensintervaller
plot(forecast_result)
# Korsvalidering - Beräkna PRESS-värdet
predicted <- forecast_result$mean
actual <- test_data
press <- sum((actual - predicted)^2)
# Skriv ut PRESS-värdet
print(press)
# Filter the data for the last 3 months
# This assumes your data is in chronological order and includes recent dates
end_date <- max(smhi_daily$Date)
start_date <- as.Date(end_date) - 90  # 90 days approximately equals 3 months
# Subset the data for this 3-month period
three_month_data <- subset(smhi_daily, Date >= start_date & Date <= end_date)
# Detrend this 3-month data if required
detrended_three_month <- diff(three_month_data$DailyAverage)
# Convert the detrended data to a time series object
# The frequency might need to be adjusted if you have fewer data points
ts_data <- ts(detrended_three_month, frequency = 365)
# Since it's a short-term model, you might not split it into training and testing sets
# But if you still want to, you can adjust the window accordingly
# For example, leaving out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Modeling with ARIMA
fit <- auto.arima(train_data)
# Since it's a short-term model, you might not split it into training and testing sets
# But if you still want to, you can adjust the window accordingly
# For example, leaving out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Modeling with ARIMA
fit <- auto.arima(train_data)
```{r echo=FALSE}
# Since it's a short-term model, you might not split it into training and testing sets
# But if you still want to, you can adjust the window accordingly
# For example, leaving out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Modeling with ARIMA
fit <- auto.arima(train_data)
checkresiduals(fit)
knitr::opts_chunk$set(echo = TRUE)
library(forecast)
library(stats)
library(ggplot2)
library(dplyr)
smhi <- read.csv("C:/Users/Jonat/Desktop/Lab 4/smhi_daily.csv", sep = ";")
# Identifiera möjliga trender och säsongsmönster
plot(smhi_daily, main = "Tidsserieplot för smhi")
pacf(smhi_daily, main = "PACF för smhi")
# avtrendifiera tidserien
detrended_smhi <- diff(smhi_daily$DailyAverage)
plot(detrended_smhi, type = 'l')
# Filter the data for the last 3 months
# This assumes your data is in chronological order and includes recent dates
end_date <- max(smhi_daily$Date)
start_date <- as.Date(end_date) - 90  # 90 days approximately equals 3 months
# Subset the data for this 3-month period
three_month_data <- subset(smhi_daily, Date >= start_date & Date <= end_date)
# Detrend this 3-month data if required
detrended_three_month <- diff(three_month_data$DailyAverage)
# Convert the detrended data to a time series object
# The frequency might need to be adjusted if you have fewer data points
ts_data <- ts(detrended_three_month, frequency = 365)
# Since it's a short-term model, you might not split it into training and testing sets
# But if you still want to, you can adjust the window accordingly
# For example, leaving out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Modeling with ARIMA
fit <- auto.arima(train_data)
checkresiduals(fit)
# Convert the detrended data to a time series object
# Adjust the frequency parameter based on your data's seasonality
ts_data <- ts(detrended_smhi, frequency = 365)
# Split the data into training and testing sets
# Assuming you want to leave out the last 14 days for testing
train_data <- window(ts_data, end = c(end(ts_data)[1], end(ts_data)[2] - 14))
test_data <- window(ts_data, start = c(end(train_data)[1], end(train_data)[2] + 1))
# Uncomment these lines once you are ready to model
fit <- auto.arima(train_data)
# Set working directory
setwd("C:/Users/Jonat/Desktop/Paper")
# Load necessary libraries
library(tidyverse)
library(lubridate)
# Load necessary libraries
install.packages("tidyverse")
install.packages("lubridate")
install.packages("zoo")
library(tidyverse)
library(lubridate)
library(zoo)
# Load the data
gs10 <- read.csv("GS10.csv")
View(gs10)
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
View(unrate)
View(recession)
View(recession)
recession <- read.csv("USRECD.csv")
View(recession)
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
gs2 <- gs2 %>% mutate(date = as.Date(DATE)) %>% select(date, GS2)
recession <- recession %>% mutate(date = as.Date(DATE)) %>% select(date, USRECD)
unrate <- unrate %>% mutate(date = as.Date(DATE)) %>% select(date, UNRATE)
indpro <- indpro %>% mutate(date = as.Date(DATE)) %>% select(date, INDPRO)
View(gs10)
View(recession)
# Merge datasets
data <- gs10 %>%
left_join(gs2, by = "date") %>%
left_join(recession, by = "date") %>%
left_join(unrate, by = "date") %>%
left_join(indpro, by = "date")
# Filter data from 1976 onwards
data <- data %>% filter(date >= as.Date("1976-06-01"))
# Calculate the yield spread
data <- data %>%
mutate(spread = GS10 - GS2)
# Handling missing values
data <- data %>%
na.locf()
View(data)
# Fit the probit model
probit_model <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = data)
# Summary of the model
summary(probit_model)
# Load additional libraries for model evaluation
library(pscl)
# Load additional libraries for model evaluation
install.packages("pscl")
library(pscl)
install.packages("pROC")
library(pROC)
# In-sample fit (pseudo R-squared)
pR2(probit_model)
# Split data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Fit the model on training data
probit_model_train <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = train_data)
# Predict on test data
test_data <- test_data %>%
mutate(predicted_prob = predict(probit_model_train, newdata = test_data, type = "response"))
# ROC curve and AUC
roc_curve <- roc(test_data$USRECD, test_data$predicted_prob)
auc(roc_curve)
plot(roc_curve)
View(probit_model_train)
# Fit the probit model
probit_model <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = data)
# Load necessary libraries for model evaluation
library(pROC)
# Predict probabilities on the test set
test_data <- test_data %>%
mutate(predicted_prob = predict(probit_model, newdata = test_data, type = "response"))
# ROC curve and AUC
roc_curve <- roc(test_data$USRECD, test_data$predicted_prob)
auc_value <- auc(roc_curve)
plot(roc_curve, main = "ROC Curve for Recession Prediction")
summary(probit_model)
pR2(probit_model)
# Assuming you've already plotted the ROC curve and calculated AUC
roc_curve <- roc(test_data$USRECD, test_data$predicted_prob)
auc_value <- auc(roc_curve)
# Print AUC value
auc_value
plot(roc_curve)
# In-sample fit (pseudo R-squared)
pR2(probit_model)
# Split data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Fit the model on training data
probit_model_train <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = train_data)
# Predict on test data
test_data <- test_data %>%
mutate(predicted_prob = predict(probit_model_train, newdata = test_data, type = "response"))
# ROC curve and AUC
roc_curve <- roc(test_data$USRECD, test_data$predicted_prob)
auc(roc_curve)
plot(roc_curve)
# Convert variables to time series objects
data_ts <- ts(data[, -1], frequency = 12)  # Exclude date column for time series analysis
View(data_ts)
install.packages("vars")
library(vars)
# Example: VAR with lag order 1 (adjust as needed)
var_model <- VAR(data_ts, p = 1)
summary(var_model)
# Example: Forecasting 12 steps ahead
var_forecast <- predict(var_model, n.ahead = 12)
View(data_ts)
cor(data_ts)
# Example: VAR with lag order 1 (adjust as needed)
var_model <- VAR(data_ts, p = 1)
summary(var_model)
# Example: Forecasting 12 steps ahead
var_forecast <- predict(var_model, n.ahead = 12)
# Summary of VAR model
var_summary <- summary(var_model)
var_summary
# Example: Forecasting 12 steps ahead
var_forecast <- predict(var_model, n.ahead = 12)
var_forecast
plot(spread)
# Load the data
gs10 <- read.csv("GS10.csv")
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
gs2 <- gs2 %>% mutate(date = as.Date(DATE)) %>% select(date, GS2)
recession <- recession %>% mutate(date = as.Date(DATE)) %>% select(date, USRECD)
# Set working directory
setwd("C:/Users/Jonat/Desktop/Paper")
# Load the data
gs10 <- read.csv("GS10.csv")
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
gs2 <- gs2 %>% mutate(date = as.Date(DATE)) %>% select(date, GS2)
recession <- recession %>% mutate(date = as.Date(DATE)) %>% select(date, USRECD)
unrate <- unrate %>% mutate(date = as.Date(DATE)) %>% select(date, UNRATE)
indpro <- indpro %>% mutate(date = as.Date(DATE)) %>% select(date, INDPRO)
# Merge datasets
data <- gs10 %>%
left_join(gs2, by = "date") %>%
left_join(recession, by = "date") %>%
left_join(unrate, by = "date") %>%
left_join(indpro, by = "date")
View(gs10)
# Load necessary libraries
#install.packages("tidyverse")
library(tidyverse)
#install.packages("lubridate")
library(lubridate)
#install.packages("zoo")
library(zoo)
# Load the data
gs10 <- read.csv("GS10.csv")
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
gs2 <- gs2 %>% mutate(date = as.Date(DATE)) %>% select(date, GS2)
# Set working directory
setwd("C:/Users/Jonat/Desktop/Paper")
# Load necessary libraries
#install.packages("tidyverse")
library(tidyverse)
#install.packages("lubridate")
library(lubridate)
#install.packages("zoo")
library(zoo)
# Load the data
gs10 <- read.csv("GS10.csv")
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
View(gs10)
# Merge datasets
data <- gs10 %>%
left_join(gs2, by = "date") %>%
left_join(recession, by = "date") %>%
left_join(unrate, by = "date") %>%
left_join(indpro, by = "date")
# Filter data from 1976 onwards
data <- data %>% filter(date >= as.Date("1976-06-01"))
View(unrate)
View(recession)
View(indpro)
View(gs2)
View(gs10)
View(recession)
View(indpro)
View(recession)
# Load necessary libraries
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("lubridate")
install.packages("zoo")
library(tidyverse)
library(lubridate)
library(zoo)
# Load the data
gs10 <- read.csv("GS10.csv")
gs2 <- read.csv("GS2.csv")
recession <- read.csv("USRECD.csv")
unrate <- read.csv("UNRATE.csv")
indpro <- read.csv("INDPRO.csv")
# Convert dates to appropriate format and select relevant columns
gs10 <- gs10 %>% mutate(date = as.Date(DATE)) %>% select(date, GS10)
gs2 <- gs2 %>% mutate(date = as.Date(DATE)) %>% select(date, GS2)
recession <- recession %>% mutate(date = as.Date(DATE)) %>% select(date, USRECD)
unrate <- unrate %>% mutate(date = as.Date(DATE)) %>% select(date, UNRATE)
indpro <- indpro %>% mutate(date = as.Date(DATE)) %>% select(date, INDPRO)
# Merge datasets
data <- gs10 %>%
left_join(gs2, by = "date") %>%
left_join(recession, by = "date") %>%
left_join(unrate, by = "date") %>%
left_join(indpro, by = "date")
# Filter data from 1976 onwards
data <- data %>% filter(date >= as.Date("1976-06-01"))
# Calculate the yield spread
data <- data %>%
mutate(spread = GS10 - GS2)
# Handling missing values
data <- data %>%
na.locf()
# Fit the probit model
probit_model <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = data)
# Summary of the model
summary(probit_model)
# Load additional libraries for model evaluation
install.packages("pscl")
library(pscl)
install.packages("pROC")
library(pROC)
# In-sample fit (pseudo R-squared)
pR2(probit_model)
# Split data into training and testing sets
set.seed(123)
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Fit the model on training data
probit_model_train <- glm(USRECD ~ spread + UNRATE + INDPRO, family = binomial(link = "probit"), data = train_data)
# Predict on test data
test_data <- test_data %>%
mutate(predicted_prob = predict(probit_model_train, newdata = test_data, type = "response"))
# ROC curve and AUC
roc_curve <- roc(test_data$USRECD, test_data$predicted_prob)
auc(roc_curve)
plot(roc_curve)
plot(data$spread)
# Plotting the spread
plot(data$date, data$spread, type = 'l', col = 'blue', xlab = 'Date', ylab = 'Yield Curve Spread')
# Adding a red horizontal line at y = 0 (zero spread)
abline(h = 0, col = 'red', lwd = 2)
